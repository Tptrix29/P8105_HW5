---
title: "HW5"
output: github_document
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r lib-import, message=FALSE}
library(tidyverse)
library(latex2exp)
```


## Problem 1
Describe the raw data. 
```{r, message=FALSE}
homicide = read_csv("./datasets/homicide-data.csv") |>
        mutate(city_state = str_c(city, state, sep = ', '))
```
There are `r nrow(homicide)` rows and `r ncol(homicide)` columns in the dataset.

**Variables**:
- `uid`: unique identifier

- `reported_date`: date of homicide accident

- `victim_last`: last name of victim

- `victim_first`: first name of victim

- `victim_race`: race name of victim

- `victim_age`: age name of victim

- `victim_sex`: sex name of victim

- `city`: city location of homicide accident

- `state`: state location of homicide accident

- `lat`: latitude location of homicide accident

- `lon`: longitude location of homicide accident

- `disposition`: disposition outcome of homicide accident
  



```{r}
homicide |> 
        group_by(city) |>
        summarise(count = n()) |>
        knitr::kable(caption = "Total number of homicides")
```

```{r}
unsolved_label = c("Closed without arrest", "Open/No arrest")
homicide |>
        filter(disposition %in% unsolved_label) |>
        group_by(city) |>
        summarise(unsolved_count = n()) |>
        knitr::kable(caption = "Total number of unsolved homicides")
```


For the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object, apply the broom::tidy to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.
```{r}
baltimore = homicide |> 
        filter(city == "Baltimore") |>
        mutate(unsolved = if_else(disposition %in% unsolved_label, 1, 0))
prop_test_obj = prop.test(sum(baltimore$unsolved), nrow(baltimore))
prop_test_df = broom::tidy(prop_test_obj)
```
Estimated Proportion: `r prop_test_df$estimate`
Confidence Interval: (`r prop_test_df$conf.low`, `r prop_test_df$conf.high`)

Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, list columns and unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.
```{r}
tidy = function(city_name, df){
        city_data = df |> 
                filter(city == city_name) |> 
                mutate(unsolved = if_else(disposition %in% unsolved_label, 1, 0))
        prop_test_obj = prop.test(sum(city_data$unsolved), nrow(city_data))
        prop_test_df = broom::tidy(prop_test_obj)
        
        tibble(
                estimate_prop = prop_test_df$estimate,
                conf_low = prop_test_df$conf.low,
                conf_high = prop_test_df$conf.high
        )
}
```

```{r}
cities = homicide$city |> unique()
test_result = tibble(
        city = cities,
        hypo_test = map(cities, tidy, df = homicide)
        ) |> 
        unnest(hypo_test)
test_result |> knitr::kable(caption = "Estimated proportion and CI of unsolved homicides")
```


Create a plot that shows the estimates and CIs for each city – check out geom_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides.

```{r}
test_result |>
        mutate(city = fct_reorder(city, estimate_prop)) |>
        ggplot(aes(x = city, y = estimate_prop, color = city), width = 100) +
        geom_point() +
        geom_errorbar(aes(ymin = conf_low, ymax = conf_high)) +
        theme_bw() +
        labs(x = "", y = "Estimated proportion", title = "Unsolved homicide proportion") +
        theme(plot.title = element_text(hjust = 0.5), axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank())
```



## Problem 2
This zip file contains data from a longitudinal study that included a control arm and an experimental arm. Data for each participant is included in a separate file, and file names include the subject ID and arm.

```{r, message=FALSE}
pth = "./datasets/data/"

data_files = tibble(
        filename = list.files(pth)
)

extract_info = function(filename, pth){
        tibble(
                read_csv(str_c(pth, filename))
        )
}

longitudial_data = data_files |>
        mutate(
                subject_id = str_extract(filename, pattern = "\\d+"),
                arm = str_extract(filename, pattern = "^[a-zA-Z]+"),
                data = map(filename, extract_info, pth)
        ) |> 
        unnest(data)
```


Make a spaghetti plot showing observations on each subject over time.
```{r}
longitudial_data |> 
        pivot_longer(starts_with("week_"), names_prefix = "week_", names_to = "week") |>
        mutate(week = as.numeric(week)) |>
        ggplot(aes(x = week, y = value, color = subject_id)) +
        geom_path() +
        theme_bw() +
        labs(x = "Week", y = "Value", title = "Longitudial Value") +
        theme(plot.title = element_text(hjust = 0.5)) + 
        facet_grid(. ~ arm)
```
The values of `con` group are fluctuating in specific range while that of `exp` group are with significant increasing trend along with the time.

## Problem 3
When designing an experiment or analysis, a common question is whether it is likely that a true effect will be detected – put differently, whether a false null hypothesis will be rejected. The probability that a false null hypothesis is rejected is referred to as power, and it depends on several factors, including: the sample size; the effect size; and the error variance.

### Dataset Generation
```{r}
trail_count = 5000
exp_data = tibble(
        mu = 1:6
) |> 
        mutate(data = map(mu, function(k){
                tibble(
                        serial = 1:trail_count,
                        hypo_test = map(1:trail_count, function(i, mu = k){
                                data = rnorm(n = 30, mean = mu, sd = 5)
                                test_obj = t.test(data)
                                broom::tidy(test_obj)
                        })
                )
                
                })
               ) |>
        unnest(data) |> 
        unnest(hypo_test)
```


### Plots
1. Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of $\mu$ on the x axis. Describe the association between effect size and power.

```{r}
exp_data |>
        filter(p.value < 0.05) |>
        group_by(mu) |>
        summarise(count = n(), rej_count= n()) |>
        ggplot(aes(x = mu, y = rej_count)) +
        geom_point(color = "steelblue") + 
        geom_path(color = "steelblue", linewidth = 1.2) +
        theme_bw() +
        labs(x = TeX("$\\mu$"), y = TeX("Rejection count"), title = TeX("Rejection count - $\\mu$")) +
        theme(plot.title = element_text(hjust = 0.5))
```
Association between effect size and power: 
In the figuer, the larger $\mu$ leads to the higher rejection count, which equals to the higher power.
So the larger the effect size, the higher the power.


2. Make a plot showing the average estimate of $\hat{\mu}$ on the y axis and the true value of $\mu$ on the x axis.
```{r}
exp_data |>
        group_by(mu) |>
        summarise(mean_estimation = mean(estimate)) |>
        ggplot(aes(x = mu)) +
        geom_line(aes(y = mu), lty = "dashed", color = "grey", linewidth = 1.2) +
        geom_point(aes(y = mean_estimation), color = "steelblue") +
        geom_path(aes(y = mean_estimation), color = "steelblue", linewidth = 1) +
        theme_bw() +
        labs(x = TeX("$\\mu$"), y = TeX("Mean estimation of $\\mu$"), title = TeX("Mean of $\\hat{\\mu}$ - $\\mu$")) +
        theme(plot.title = element_text(hjust = 0.5))
```

3. Make a second plot (or overlay on the first) the average estimate of $\hat{\mu}$ only in samples for which the null was rejected on the y axis and the true value of $\mu$ on the x axis. 
```{r}
exp_data |>
        filter(p.value < 0.05) |>
        group_by(mu) |>
        summarise(estimate_mean_rej = mean(estimate)) |>
        ggplot(aes(x = mu)) +
        geom_line(aes(y = mu), lty = "dashed", color = "grey", linewidth = 1.2) +
        geom_point(aes(y = estimate_mean_rej), color = "steelblue") +
        geom_path(aes(y = estimate_mean_rej), color = "steelblue", linewidth = 1) + 
        theme_bw() +
        labs(x = TeX("$\\mu$"), y = TeX("Mean estimation of $\\mu$ within rejected samples"), title = TeX("Mean of $\\hat{\\mu}$(Rejection) - $\\mu$")) +
        theme(plot.title = element_text(hjust = 0.5))
```


The sample average of $\hat{\mu}$ across tests for which the null is rejected gradually equal to the true value of $\mu$ in approximation along with the increasing of $\mu$.

Reason: 

